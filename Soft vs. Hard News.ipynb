{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dfe364-388d-4ef1-90fe-b3af50c0818c",
   "metadata": {},
   "source": [
    "# Basic Dictionary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd521bab-152f-40b2-9bd0-1d65b521087e",
   "metadata": {},
   "source": [
    "In the following, I attempt to distinguish between hard news (i.e. news focused on current events, politics, public affairs etc.) and soft news (i.e. news focused on lifestyle, entertainment, human interest stories etc.) using a simple dictionary method. I define two dictionaries, one for hard and one for soft news, in order to classify the articles in the dataset. The dictionaries are based on a brief qualitative immersion phase in which I identify some of the words that may identify soft vs. hard news. Then I use keyword expansion with word2vec to broaden their scope. The classification categories consist of 'soft news' meaning there were words from the soft word dictionary, 'hard news' meaning there were more words from the hard news dictionary, 'mixed' meaning there were equal words from both dictionaries and 'none' meaning there were no words from either dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e16e9c-94ee-4c03-9bed-a09ed79416f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Type Proportions:\n",
      " news_type\n",
      "hard     0.500\n",
      "soft     0.389\n",
      "mixed    0.078\n",
      "none     0.034\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('bbc_news.csv')  \n",
    "\n",
    "# Define initial keyword dictionaries\n",
    "soft_news = {'gucci', 'disney', 'world cup', 'celebrity', 'football', 'arsenal', 'liverpool', 'wsl', 'music',\n",
    "             'song', 'fashion', 'squad', 'dance', 'artist', 'coffee', 'f1', 'royal', 'star', 'film', 'travel',\n",
    "             'novel', 'book', 'author', 'chef', 'lifestyle', 'healthy', 'actor', 'surfing', 'athlete', 'paralympic',\n",
    "             'medal', 'league', 'sci fi', 'DJ'}\n",
    "\n",
    "hard_news = {'war', 'ukraine', 'covid', 'restrictions', 'tariffs', 'politics', 'zelensky', 'tory', 'russia',\n",
    "             'murder', 'civilians', 'died', 'assault', 'killed', 'putin', 'tornado', 'climate change', 'emissions',\n",
    "             'elections', 'business', 'prisoner', 'arrest', 'farmer', 'crash', 'supply', 'abortion', 'farming',\n",
    "             'EU', 'terrorism'}\n",
    "\n",
    "# Combine 'title' and 'description' into a single text field\n",
    "df['full_text'] = df['title'].astype(str) + \" \" + df['description'].astype(str)\n",
    "\n",
    "# Tokenize and lowercase the combined text\n",
    "df['tokens'] = df['full_text'].apply(lambda x: [w.lower() for w in word_tokenize(x)])\n",
    "\n",
    "# Train Word2Vec model on tokenized descriptions\n",
    "model = Word2Vec(sentences=df['tokens'], vector_size=100, window=2, \n",
    "                 min_count=1, sg=1, epochs=5, seed=42) # set sg=1, min_count=1, window=2 because the texts are short and seed for consistency\n",
    "\n",
    "# Function to expand keywords\n",
    "def expand_keywords_w2v(model, seed_words, topn=100, threshold=0.6):\n",
    "    expanded = set(seed_words)\n",
    "    for word in seed_words:\n",
    "        try:\n",
    "            similar = model.wv.most_similar(word, topn=topn)\n",
    "            for sim_word, score in similar:\n",
    "                if score >= threshold:\n",
    "                    expanded.add(sim_word.lower())\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return expanded\n",
    "\n",
    "# Expand both categories\n",
    "soft_news_expanded = expand_keywords_w2v(model, soft_news)\n",
    "hard_news_expanded = expand_keywords_w2v(model, hard_news)\n",
    "\n",
    "# Classification of Articles\n",
    "def classify_article(tokens, soft_set, hard_set):\n",
    "    soft_hits = sum(1 for w in tokens if w in soft_set)\n",
    "    hard_hits = sum(1 for w in tokens if w in hard_set)\n",
    "    if soft_hits > hard_hits:\n",
    "        return 'soft'\n",
    "    elif hard_hits > soft_hits:\n",
    "        return 'hard'\n",
    "    elif soft_hits == hard_hits and soft_hits > 0:\n",
    "        return 'mixed'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "# Apply classification\n",
    "df['news_type'] = df['tokens'].apply(lambda x: classify_article(x, soft_news_expanded, hard_news_expanded))\n",
    "\n",
    "# Proportion\n",
    "proportion = df['news_type'].value_counts(normalize=True).round(3)\n",
    "print(\"News Type Proportions:\\n\", proportion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
